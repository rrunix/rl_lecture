{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, namedtuple\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import train_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aprendizaje por refuerzo\n",
    "\n",
    "**DSLAB - Rubén Rodríguez Fernández**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Qué hemos visto hasta ahora en aprendizaje máquina:\n",
    "\n",
    " * Aprendizaje **supervisado**: Las observaciones, $X$, tienen asociada una etiqueta, $y$, y el objetivo es encontrar un función $f: X \\mapsto y$. Por ejemplo, clasificación y regresión.\n",
    " * Aprendizaje **no-supervisado**: Las observaciones , $X$, no tienen una etiqueta asociada y el objetivo es encontrar relaciones entre las observaciones. Por ejemplo, clustering y estimación de densidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Aprendizaje **reforzado**: \n",
    "\n",
    "* Un agente interactua con el entorno a través de acciones y obtiene una recompensa. \n",
    "* El objetivo es maximizar esa recompensa. \n",
    "* La recompensa no tiene por que ser inmediata, por lo que un agente puede realizar varias acciones antes de recibir una recompensa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ejemplos de aprendizaje reforzado:\n",
    "\n",
    "* Juegos - El agente, jugador, trata de ganar (recompensa positiva) sin perder (recompensa negativa). P.ej, AlphaGo Zero. \n",
    "* Invertir en valores - Ganar dinero (positivo), perder (negativo)\n",
    "* Planning - Elaborar una ruta. La ruta puede estar penalizada (negativa) por el número de pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ejemplos de recompensas:\n",
    "\n",
    "* Correr un maratón: El objetivo es llegar lo antes posible (penalización por tiempo). Alimentarse o hidratarse hace que tengamos una penalización a corto plazo, pero evitará que perdamos más tiempo a largo plazo.\n",
    "* Jugar al tenis: Hacer un punto (recompensa positiva), que te hagan un punto (negativa).\n",
    "* Estudiar para un examen: A corto plazo, estudiar puede tener una recompensa negativa porque no nos guste, pero a largo plazo aprobaremos el examen y no tendremos que estudiar (recompensa positiva)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¿Cómo el **agente** realiza la **interacción** con el **entorno**? La interacción se lleva a cabo en timesteps discretos, $ t = 0, 1, \\ldots $. En cada iteración se realizan las siguientes tareas:\n",
    "\n",
    "* El agente recibe una representación del estado actual del entorno, $s_t^a$,\n",
    "* que tiene la información necesaria para realizar la acción, $a_t$, donde $a_t$ pertenece al conjunto de acciones disponibles en el estado $s_t$, $a_t \\in A(s_t^a)$\n",
    "* y obtiene una recompensa $r_t$ ($r \\in \\mathbb{R}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Representación gráfica de la interacción de un agente con el entorno.\n",
    "\n",
    "<img src=\"images/reinforcement_learning_diagram.png\" alt=\"rl diagram\" width=\"300\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El estado del entorno, $s_t^e$, puede ser igual o contener más información que el estado que recibe el agente, $s_t^a$, dando lugar a dos tipos de entornos:\n",
    " * **Observable**: El agente es capaz de percibir toda la información del entorno, $s_t^a = s_t^e$. Por ejemplo, en el ajedrez el entorno es el tablero y la colocación de las piezas, en este caso un agente (jugador) es capaz de percibir todo el entorno.\n",
    " * **Parcialmente observable**: El agente es capaz de observar solo una parte del entorno $s_t^a \\ne s_t^e$. Por ejemplo, en el juego flappy birds un agente (jugador) sólo percibe el entorno que se muestra en la pantalla, pero no tiene información sobre los obstaculos que hay después. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **historia** de un agente, $H_t$, es el secuencia de estados, acciones y recompensas hasta el instante $t$. $H_t = s_0, a_0, r_0, \\ldots, s_t, a_t, r_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Objetivo**: máximizar la secuencia cumulativa de recompensas:\n",
    "\n",
    "* Según la hipótesis de la recompensa (Rewards hypothesis), todos los objetivos pueden ser descritos como la maximización de la esperanza cumulativa de la recompensa. \n",
    "* En algunos casos es mejor sacrificar las recompensas a corto plazo para obtener una recompensa mayor a largo plazo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Markov decision process\n",
    "\n",
    "* Un proceso de decision de Markov (MDP) proporciona un marco de trabajo matemático para sistemas de tomas de decisión donde hay aleatoriedad en las acciones tomadas. \n",
    "* Es un enfoque para algunos problemas de aprendizaje por refuerzo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Un MDP está compuesto por los siguientes componentes:\n",
    "\n",
    "* El estado, $S$,\n",
    "* El conjunto de acciones, $A$, y las acciones disponibles para el estado $s$, $A(s)$,\n",
    "* La probabilidad de transición $T(s,a,s\\prime) \\sim \\mathbb{P}(s\\prime|s,a)$, que es la probabilidad de acabar en el estado $s\\prime$ al realizar la acción $a$ en el estado $s$,\n",
    "* Y la recompensa $R(s,a,s\\prime)$ que es la recompensa esperada al pasar de $s$ a $s\\prime$ a través del la accción $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Además, se tiene que cumplir la propiedad de Markov:\n",
    "\n",
    "$\\mathbb{P}(S_{t+1}|S_t) = \\mathbb{P}(S_{t+1}|S_t, \\ldots, S_0)$\n",
    "\n",
    "que implica que el estado $s_t$ captura toda la información relevante de la historia (el **futuro** es independiente del **pasado** dado el **presente**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Objetivo**: Encontrar una policy $\\pi$ que permita seleccionar la acción a tomar, $a_t$, en el estado $s_t$ ($\\pi(s_t) = a_t$) que maximize una función de la secuencia cumulativa de recompensas.\n",
    "\n",
    "Normalmente, esta función de la secuencia cumulativa de recompensas es la esperanza de la suma de recompensas descontada sobre un horizonte infinito:\n",
    "\n",
    "$\\mathbb{E}[\\sum^\\infty_{t=0} \\gamma^t R(s_t,a_t,s_{t+1})]$\n",
    "\n",
    "Donde $\\gamma$ es el factor de descuento tal que $0 \\le \\gamma \\le 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El factor de descuento es un parámetro muy importante, que permite establecer cuan **lejos** el método tiene que mirar en el horizonte de recompensas:\n",
    "\n",
    "* Un factor de descuento cercano a cero indica que solo las recompensas **inmediatas** son consideradas, \n",
    "* Un factor de descuento a uno permite considerar un **horizonte mayor**. \n",
    "\n",
    "¿Cuál es el mejor factor de descuento? Como en la mayoría de los parámetros en ML, esto dependerá del problema. En la práctica, en la mayoría de los casos se elige $\\gamma = 0.9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Definiciones**: \n",
    "\n",
    "La policy óptima, $\\pi^*$, se representa como :\n",
    "\n",
    "$\\pi^* = argmax_\\pi \\mathbb{E} [\\sum^\\infty_{t=0} \\gamma^t R(s_t) |\\pi]$\n",
    "\n",
    "Con esta información podemos definir la función Value, $V(s)$, que define la recompensa esperada en el futuro empezando desde el estado $s$ siguiendo la política $\\pi$:\n",
    "\n",
    "$V^\\pi(s) = \\mathbb{E}[\\sum^\\infty_{t=0} \\gamma^t R(s_t) |\\pi, s_0=s]$\n",
    "\n",
    "y podemos escribir la política óptima en términos de $V(s)$:\n",
    "\n",
    "$\\pi^* = argmax_a \\sum_{s\\prime} T(s, a, s\\prime) * V^{\\pi^*} (s\\prime)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Métodos para resolver problemas de aprendizaje reforzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Para visualizar como funcionan los algoritmos que vamos a ver con un problema de aprendizaje reforzado llamado FrozenLake-v0:\n",
    "\n",
    "<img src=\"images/frozen_lake.png\" alt=\"frozen lake\" width=\"300\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Descripción del juego**: \n",
    "* caminar por un lago helado (F) para llegar a la meta (G), \n",
    "* evitando caerse al agua en las zonas donde la capa de hielo es menor (nos caeríamos) (H). \n",
    "* los nodos terminales, que indican que el juego se acaba, son G y H.\n",
    "* Las acciones que puede realizar el agente son moverse a la izquierda, derecha, arriba y abajo. \n",
    "* El número de estados son 16, y representa cada una de las celdas del grid.\n",
    "\n",
    "Fuente imagen: \n",
    "https://academy.dataiku.com/latest/tutorial/machine-learning/reinforcement-learning-q-learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Breve introduducción al paquete gym de Openai**\n",
    "\n",
    "Lo primero que tenemos que hacer es crear un entorno (ver otros entornos en https://gym.openai.com/), y resetearlo para llevarlo al estado original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=True)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Para visualizar el entorno utilizamos la función render. Esta función tiene un parámetro **mode** que nos permitirá utilizar diferentes tipos de representaciones como texto o gráfica, en el entorno frozenlake solo tenemos disponible la representación basada en texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El conjunto de posibles acciones se encuentra en el atributo **action_space**. En FrozenLake, solo disponemos de cuatro acciones: izquierda, abajo, derecha y arriba (0, 1, 2, 3) respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El número de posibles estados se encuentra el atributo **nS**. En este problema son 16 (es un grid de 4x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.nS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para realizar una acción utilizamos el método **step**, que nos devuelve el nuevo estado, recompensa, un flag que nos indica si el juego ha terminado o no, e información sobre la transición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0 False\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(2) \n",
    "print(state, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prob': 0.3333333333333333}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si visualizamos de nuevo el entorno veremos como el agente se ha movido de la celda inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una vez terminado el entrenamiento podemos cerrar el entorno con el método **close**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Programación dinámica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dp_params = {\n",
    "    'discount_factor': {'values': np.arange(0, 1.1, 0.1), 'default': 1.0, 'text': 'discount_factor'}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Value iteration es un método para calcular la policy y funcion value óptima para un MDP,\n",
    "* Este método parte de la premisa que la función value, $V(s)$, es suficiente para aprender la policy óptima. \n",
    "* Por lo tanto, este método primero cálcula $V(s)$, y posteriormente $\\pi^*$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El método de value iteration utiliza la definición de $V(s) \\equiv V^\\pi(s)$, con las definiciones anteriores llegando a:\n",
    "\n",
    "$V(s) = R(s) + \\gamma max_a \\sum_{s\\prime} T(s, a, s\\prime) V(s\\prime)$\n",
    "\n",
    "Con esta definición de $V(s)$, value iteration se define como:\n",
    "\n",
    "$V_{t+1}(s) = R(s) + \\gamma max_a \\sum_{s\\prime} T(s, a, s\\prime) V_t(s\\prime)$\n",
    "\n",
    "donde se empieza con una función valor aleatoria, $V_0$, y se actualiza hasta que converja. En la práctica, el algoritmo se detiene cuando los cambios de la función valor son menores a un umbral, en este caso theta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El psudo-código de value iteration es el siguiente (fuente http://incompleteideas.net/book/bookdraft2017nov5.pdf):\n",
    "\n",
    "<img src=\"images/value_iteration.png\" alt=\"value iteration\" width=\"600\" class=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def one_step_lookahead(env, state, V, discount_factor):\n",
    "        \"\"\"\n",
    "        CODE FROM (code might be modified): https://github.com/dennybritz/reinforcement-learning\n",
    "        \n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            A[a] = sum([prob * (reward + discount_factor * V[next_state]) \n",
    "                        for prob, next_state, reward, done in env.P[state][a]])\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0, return_history=True):\n",
    "    \"\"\"\n",
    "    CODE FROM (code might be modified): https://github.com/dennybritz/reinforcement-learning\n",
    "    \n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "\n",
    "    V_history = []\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        # Update each state...\n",
    "        for s in range(env.nS):\n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = one_step_lookahead(env, s, V, discount_factor)\n",
    "            best_action_value = np.max(A)\n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = best_action_value\n",
    "        \n",
    "        if return_history:\n",
    "            V_history.append(V.copy())\n",
    "            \n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = one_step_lookahead(env, s, V, discount_factor)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[s, best_action] = 1.0\n",
    "\n",
    "    if return_history:\n",
    "        return policy, V, V_history\n",
    "    else:\n",
    "        return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Demostración de value iteration en custom frozenlake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c51f41a1a0d84c20b5205508a3a910d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectionSlider(continuous_update=False, description='discount_factor', index=10, option…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_func = train_utils.multi_display(train_utils.display_lake_policy, None, train_utils.display_lake_value_history)\n",
    "train_utils.display_learning_widget(value_iteration, dp_params, plot_func, env_params=train_utils.custom_env_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a probar a jugar una partida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_env = gym.make('custom-FrozenLake-v0', prob_action=0.8, rg_hole=0, rg_floor=0)\n",
    "p_frozen, v_frozen = value_iteration(frozen_env, return_history=False, discount_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards 1\n"
     ]
    }
   ],
   "source": [
    "train_utils.play_episode(frozen_env, p_frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>↓</td><td>↑</td><td>↑</td><td>↑</td></tr>\n",
       "<tr><td>←</td><td>H</td><td>↑</td><td>H</td></tr>\n",
       "<tr><td>↑</td><td>↓</td><td>←</td><td>H</td></tr>\n",
       "<tr><td>H</td><td>→</td><td>↓</td><td>G</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_utils.display_lake_policy(frozen_env, p_frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb4595b0390>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_utils.display_lake_value(frozen_env, v_frozen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy iteration es un método para calcular la política óptima para un MDP. El algoritmo está compuesto por dos métodos:\n",
    "\n",
    "* policy evaluation: se calcula la función value (también llamada state-value) para una policy arbitraria.\n",
    "* policy improvement: se mejora la policy utilizando la función value del paso anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Evaluation**:\n",
    " * Calculamos la función value utilizando una policy $\\pi$ con las siguientes ecuaciones:\n",
    "\n",
    "$V_\\pi(s) = \\mathbb{E}[G_t | S_t = s]$\n",
    "$= \\mathbb{E}[R_{t+1} + \\gamma V_\\pi(s_{t+1}) | S_t = s]$\n",
    "$= \\sum_{a} \\pi(a|s) \\sum_{s\\prime, r} p(s, a, s\\prime) * [r + \\gamma V(s\\prime)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Improvement**:\n",
    "\n",
    "* Una vez hemos calculado la función value $V_\\pi(s)$ podemos utilizarla para mejorar la policy. \n",
    "* Para mejorar la política, se compara la acción seleccionada para un estado usando la política con la mejor acción utilizando la función valor, y en caso de ser diferentes, se actualiza la política."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "El pseudo-código de policy iteration es el siguiente (fuente http://incompleteideas.net/book/bookdraft2017nov5.pdf):\n",
    "\n",
    "<img src=\"images/policy_iteration.png\" alt=\"policy iteration\" width=\"600\" class=\"center\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.001):\n",
    "    \"\"\"\n",
    "    CODE FROM (code might be modified): https://github.com/dennybritz/reinforcement-learning\n",
    "    \n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # For each action, look at the possible next states...\n",
    "                for  prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # Calculate the expected value\n",
    "                    v += action_prob * prob * (reward + discount_factor * V[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    CODE FROM (code might be modified): https://github.com/dennybritz/reinforcement-learning\n",
    "    \n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI environment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    while True:\n",
    "        # Evaluate the current policy\n",
    "        V = policy_eval_fn(policy, env, discount_factor)\n",
    "        \n",
    "        # Will be set to false if we make any changes to the policy\n",
    "        policy_stable = True\n",
    "        \n",
    "        # For each state...\n",
    "        for s in range(env.nS):\n",
    "            # The best action we would take under the current policy\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "            \n",
    "            # Find the best action by one-step lookahead\n",
    "            # Ties are resolved arbitarily\n",
    "            action_values = one_step_lookahead(env, s, V, discount_factor)\n",
    "            best_a = np.argmax(action_values)\n",
    "            \n",
    "            # Greedily update the policy\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(env.nA)[best_a]\n",
    "        \n",
    "        # If the policy is stable we've found an optimal policy. Return it\n",
    "        if policy_stable:\n",
    "            return policy, V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Demostración de policy iteration en custom frozenlake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6720ab86d42a42539daa31a4956e1f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectionSlider(continuous_update=False, description='discount_factor', index=10, option…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_func = train_utils.multi_display(train_utils.display_lake_policy, None, train_utils.display_lake_value_history)\n",
    "train_utils.display_learning_widget(policy_improvement, dp_params, plot_func, env_params=train_utils.custom_env_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monte Carlo\n",
    "\n",
    "* Los métodos vistos hasta ahora de programación dinámica tienen una gran limitación, ya que asumen que se tiene un conocimiento completo del entorno,\n",
    "* En la práctica, esto no es siempre es así.\n",
    "* Los métodos de monte carlo solucionan este problema aprendiendo del entorno a través de la experiencia generada al interaccionar con el entorno.\n",
    "* Estos métodos tienen una gran varianza, pero no está sesgados. \n",
    "* Convergencia asintótica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¿Cómo se definen la experiencia?\n",
    "\n",
    "* La experiencia se agrupa en episodios (en el caso de los juegos, un episodio podría ser una partida),\n",
    "* cada episodio esta compuesto de una secuencia ordenada de la 4-upla (estado, acción, recompensa y proximo estado)\n",
    "* utilizando esta experiencia, podemos calcular la función state-action\n",
    "* y mejorarla en cada episodio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "¿Cómo estimamos la policy?\n",
    "\n",
    "* Generamos una función state-action $Q(s,a)$ aleatoria,\n",
    "* y creamos una policy $\\pi$ utilizando $Q(s,a)$,\n",
    "* Posteriormente, generamos un episodio siguiendo la policy $\\pi$,\n",
    "* Para cada estado y acción $s, a$ en el episodio, buscamos la primera vez que aparecen en el episodio,\n",
    "* y calculamos la recompensa cumulativo desde ese punto,\n",
    "* actualizamos $Q(s, a)$ con la media de todas las recompensas cumulativas para $s, a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Psuedo código del método montecarlo para estimar $\\pi$ (fuente http://incompleteideas.net/book/bookdraft2017nov5.pdf):\n",
    "\n",
    "<img src=\"images/montecarlo.png\" alt=\"montecarlo\" width=\"600\" class=\"center\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, num_episodes, episode_steps=100, discount_factor=1.0, epsilon=0.1, encode_state=None):\n",
    "    \"\"\"\n",
    "    CODE FROM (code might be modified): https://github.com/dennybritz/reinforcement-learning\n",
    "    \n",
    "    Monte Carlo Control using Epsilon-Greedy policies.\n",
    "    Finds an optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "        encode_state: function to change the state representation\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, policy).\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "        policy is a function that takes an observation as an argument and returns\n",
    "        action probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of sum and count of returns for each state\n",
    "    # to calculate an average. We could use an array to save all\n",
    "    # returns (like in the book) but that's memory inefficient.\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.ones(env.action_space.n) / env.action_space.n)\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in tqdm(range(1, num_episodes + 1)):\n",
    "        # Generate an episode.\n",
    "        # An episode is an array of (state, action, reward) tuples\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        for t in range(episode_steps):\n",
    "            if encode_state:\n",
    "                next_state = encode_state(next_state)\n",
    "                \n",
    "            probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        # Find all (state, action) pairs we've visited in this episode\n",
    "        # We convert each state to a tuple so that we can use it as a dict key\n",
    "        sa_in_episode = set([(x[0], x[1]) for x in episode])\n",
    "        for state, action in sa_in_episode:\n",
    "            sa_pair = (state, action)\n",
    "            # Find the first occurance of the (state, action) pair in the episode\n",
    "            first_occurence_idx = next(i for i,x in enumerate(episode)\n",
    "                                       if x[0] == state and x[1] == action)\n",
    "            # Sum up all rewards since the first occurance\n",
    "            G = sum([x[2]*(discount_factor**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
    "            # Calculate average return for this state over all sampled episodes\n",
    "            returns_sum[sa_pair] += G\n",
    "            returns_count[sa_pair] += 1.0\n",
    "            Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]\n",
    "        \n",
    "        # The policy is improved implicitly by changing the Q dictionary\n",
    "    \n",
    "    return dict(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "mc_params = {\n",
    "    'discount_factor': {'values': np.arange(0, 1.1, 0.1), 'default': 1.0, 'text': 'discount_factor'},\n",
    "    'epsilon': {'values': np.arange(0, 1.1, 0.1), 'default': 1.0, 'text': 'epsilon'},\n",
    "    'num_episodes': {'values': (10**3, 10**4, 3 * 10**4, 6 * 10**4, 10**5, 10**6, 10**7), 'default': 10**4, 'text': 'num_episodes'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Demostración de Monte Carlo en frozenlake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e07e71664b4501bed647e174fda38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectionSlider(continuous_update=False, description='discount_factor', index=10, option…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_func = train_utils.multi_plot(train_utils.display_lake_q, train_utils.display_lake_qpolicy)\n",
    "train_utils.display_learning_widget(mc_control_epsilon_greedy, mc_params, plot_func, env_params=train_utils.custom_env_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Temporal-difference Learning\n",
    "\n",
    "* TD-Learning es una combinación de las técnicas Monte Carlo y programación dinámica,\n",
    "* Como montecarlo, TD funciona con las experiencias y no requiere de un modelo del entorno\n",
    "* Al igual que la programación dinámica, TD utiliza bootstrapping para hacer las actualizaciones.\n",
    "* La convergencia de TD suele aprender más rápido pero están sesgados (a las condiciones iniciales),\n",
    "* Por otra parte, los métodos basados en Monte Carlo no están sesgados pero requieren de más iteraciones para llegar a un resultado similar a TD.\n",
    "\n",
    "<!-- \n",
    "The main problem with TD learning and DP is that their step updates are biased on the initial conditions of the learning parameters. The bootstrapping process typically updates a function or lookup Q(s,a) on a successor value Q(s',a') using whatever the current estimates are in the latter. Clearly at the very start of learning these estimates contain no information from any real rewards or state transitions.\n",
    "\n",
    "If learning works as intended, then the bias will reduce asymptotically over multiple iterations. However, the bias can cause significant problems, especially for off-policy methods (e.g. Q Learning) and when using function approximators. That combination is so likely to fail to converge that it is called the deadly triad in Sutton & Bart.\n",
    "\n",
    "Monte Carlo control methods do not suffer from this bias, as each update is made using a true sample of what Q(s,a) should be. However, Monte Carlo methods can suffer from high variance, which means more samples are required to achieve the same degree of learning compared to TD.\n",
    "\n",
    "In practice, TD learning appears to learn more efficiently if the problems with the deadly triad can be overcome. Recent results using experience replay and staged \"frozen\" copies of estimators provide work-arounds that address problems - e.g. that is how DQN learner for Atari games was built. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las actualizaciones en TD-Learning tienen normalmente la siguiente forma:\n",
    "    \n",
    "* $V(s_t) = V(s_t) + \\alpha[R_{t+1} + \\gamma V(s_{t+1}) - V(s_{t})]$,\n",
    "* Estimacion = Valor actual + $\\alpha$ [Nuevo valor - Valor actual],\n",
    "* donde $0 \\le \\alpha \\le 1$  es el parámetro de aprendizaje que regula las actualizaciones.\n",
    "* El término $R_{t+1} + \\gamma V(s_{t+1}) - V(s_{t})$ es el \"error\", de la estimación $V(s_{t})$ y la mejor estimación $R_{t+1} + \\gamma V(s_{t+1})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En las siguientes secciones vamos a ver dos métodos basados en temporal-difference, que tienen la siguiente forma:\n",
    "\n",
    "* El objetivo es calcular la función state-action ($Q(S, A)$)\n",
    "* El método se repite para un cojunto de episodios predeterminado:\n",
    "* Dentro del episodio, el método escoge una acción $a$ utilizando una policy derivada de $Q(S, A)$.\n",
    "* Ejecuta la acción, obteniendo el nuevo estado $s\\prime$, y la recompensa $r$,\n",
    "* y actualizan $Q(S, A)$.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Las diferencias de los dos métodos radican en cómo se actualiza $Q(S, A)$:\n",
    "\n",
    "* SARSA: Es un método on-policy. La actualización de los q-values se realiza utilizando la policy actual.\n",
    "* Q-Learning: Es un método off-policy. La actualización de los q-values se realiza utilizando el siguiente estado $s\\prime$ y la acción $a\\prime$ utilizando un enfoque greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "td_params = {\n",
    "    'discount_factor': {'values': np.arange(0, 1.1, 0.1), 'default': 1.0, 'text': 'discount_factor'},\n",
    "    'alpha': {'values': np.arange(0, 1.1, 0.1), 'default': 0.5, 'text': 'alpha'},\n",
    "    'epsilon': {'values': np.arange(0, 1.1, 0.1), 'default': 0.1, 'text': 'epsilon'},\n",
    "    'num_episodes': {'values': (10**3, 10**4,  3 * 10**4, 6 * 10**4, 10**5, 10**6, 10**7), 'default': 10**4, 'text': 'num_episodes'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SARSA\n",
    "\n",
    "* SARSA es un método on-policy, por lo que la función $Q(S,A)$ se actualiza utilizando la policy actual.\n",
    "* En la actualización, el método elige una acción $a\\prime$ para el estado $s\\prime$ utilizando la policy derivada de $Q(S,A)$,\n",
    "* y actualiza $Q(s,a) =  Q(s, a) + \\alpha[R + \\gamma Q(s\\prime, a\\prime) - Q(s, a)]$.\n",
    "* Recordar que tiene una forma similar al \"error\" en TD.\n",
    "* La acción $a\\prime$ -> $a$, y $s\\prime$ -> $s$, y pasamos a la siguiente iteración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Psuedo código del método SARSA (fuente http://incompleteideas.net/book/bookdraft2017nov5.pdf):\n",
    "\n",
    "<img src=\"images/sarsa.png\" alt=\"sarsa\" width=\"600\" class=\"center\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q-Learning es un método off-policy, por lo que la función $Q(S,A)$ se actualiza utilizando un enfoque greedy.\n",
    "* En la actualización, el método elige una acción $a\\prime$ para el estado $s\\prime$ utilizando un enfoque greedy,\n",
    "* y actualiza $Q(s,a) =  Q(s, a) + \\alpha[R + \\gamma max_{a\\prime} Q(s\\prime, a\\prime) - Q(s, a)]$.\n",
    "* Recordar que tiene una forma similar al \"error\" en TD.\n",
    "* El estado $s\\prime$ -> $s$, y pasamos a la siguiente iteración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Psuedo código del método q learning (fuente http://incompleteideas.net/book/bookdraft2017nov5.pdf):\n",
    "\n",
    "<img src=\"images/qlearning.png\" alt=\"q learning\" width=\"600\" class=\"center\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementaciones de SARSA y Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1, encode_state=None):\n",
    "    \"\"\"\n",
    "    CODE FROM (code might be modified): https://github.com/dennybritz/reinforcement-learning\n",
    "    \n",
    "    SARSA algorithm: On-policy TD control. Finds the optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "        encode_state: function to change the state representation\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, stats).\n",
    "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        \n",
    "        # Reset the environment and pick the first action\n",
    "        state = env.reset()\n",
    "        if encode_state:\n",
    "            state = encode_state(state)\n",
    "            \n",
    "        action_probs = policy(state)\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            # Take a step\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if encode_state:\n",
    "                next_state = encode_state(next_state)\n",
    "            \n",
    "            # Pick the next action\n",
    "            next_action_probs = policy(next_state)\n",
    "            next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)\n",
    "            \n",
    "            # TD Update\n",
    "            td_target = reward + discount_factor * Q[next_state][next_action]\n",
    "            td_delta = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_delta\n",
    "    \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            action = next_action\n",
    "            state = next_state        \n",
    "\n",
    "    return dict(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Demostración de sarsa en frozenlake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec18ced509754832984f5b30e63edd9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectionSlider(continuous_update=False, description='discount_factor', index=10, option…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_func = train_utils.multi_plot(train_utils.display_lake_q, train_utils.display_lake_qpolicy)\n",
    "train_utils.display_learning_widget(sarsa, \n",
    "                                    td_params, \n",
    "                                    plot_func, \n",
    "                                    env_params=train_utils.custom_env_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1, encode_state=None):\n",
    "    \"\"\"\n",
    "    CODE FROM (code might be modified): https://github.com/dennybritz/reinforcement-learning\n",
    "    \n",
    "    Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy\n",
    "    while following an epsilon-greedy policy\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Chance to sample a random action. Float between 0 and 1.\n",
    "        encode_state: function to change the state representation\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, episode_lengths).\n",
    "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n)) \n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        # Reset the environment and pick the first action\n",
    "        state = env.reset()\n",
    "\n",
    "        if encode_state:\n",
    "            state = encode_state(state)\n",
    "        \n",
    "        # One step in the environment\n",
    "        # total_reward = 0.0\n",
    "        for t in itertools.count():        \n",
    "            # Take a step\n",
    "            action_probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if encode_state:\n",
    "                next_state = encode_state(next_state)\n",
    "            \n",
    "            # TD Update\n",
    "            best_next_action = np.argmax(Q[next_state])    \n",
    "            td_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "            td_delta = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_delta\n",
    "                 \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state   \n",
    "    return dict(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Demostración de Q-Learning en frozenlake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08bd9ada7a94e4d836a2cae5d97cce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(SelectionSlider(continuous_update=False, description='discount_factor', index=10, option…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_func = train_utils.multi_plot(train_utils.display_lake_q, train_utils.display_lake_qpolicy)\n",
    "train_utils.display_learning_widget(q_learning, td_params, plot_func, env_params=train_utils.custom_env_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otros entornos de gym para prácticar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxi env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**:\n",
    "    There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "    \n",
    "**Observations**: \n",
    "There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations. \n",
    "    \n",
    "**Passenger locations**:\n",
    "- 0: R(ed)\n",
    "- 1: G(reen)\n",
    "- 2: Y(ellow)\n",
    "- 3: B(lue)\n",
    "- 4: in taxi\n",
    "    \n",
    "**Destinations**:\n",
    "- 0: R(ed)\n",
    "- 1: G(reen)\n",
    "- 2: Y(ellow)\n",
    "- 3: B(lue)\n",
    "        \n",
    "**Actions**:\n",
    "There are 6 discrete deterministic actions:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east \n",
    "- 3: move west \n",
    "- 4: pickup passenger\n",
    "- 5: dropoff passenger\n",
    "    \n",
    "**Rewards**: \n",
    "There is a reward of -1 for each action and an additional reward of +20 for delivering the passenger. There is a reward of -10 for executing actions \"pickup\" and \"dropoff\" illegally.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R:\u001b[43m \u001b[0m| : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_env.render()\n",
    "taxi_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podéis utilizar la función cached_call que persite las ejecuciones entre sesiones para probar diferentes combinaciones, tiene como parámetros la función que vais utilizar para resolver el problema, el nombre del entorno, un diccionario con los argumentos para crear el entorno, y otro diccionario con los parámetros del método. El resultado es una tupla con el resultado/s del método, y el entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532823b850dd4107b811ac119b52a374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Q_mc, taxi_env = train_utils.cached_call(q_learning, 'Taxi-v3', {}, method_kwargs={'num_episodes':10**4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podéis utilizar la función directamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q_mc = q_learning(taxi_env, num_episodes=10**4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para reproducir un episodio podéis utilizar la función play_episode, que recibe como parámetro el entorno y una policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards 20\n"
     ]
    }
   ],
   "source": [
    "train_utils.play_episode(taxi_env, train_utils.get_policy_from_q(Q_mc, taxi_env.nS, taxi_env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MountainCar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver descripción del entorno en https://gym.openai.com/envs/MountainCar-v0/ y https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_env = gym.make('MountainCar-v0')\n",
    "car_env.reset()\n",
    "car_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCairStateEncoder:\n",
    "    \n",
    "    def __init__(self, env, num_buckets_v = 20, num_buckets_x = 20):\n",
    "        self.env = env\n",
    "        self.num_buckets_v = num_buckets_v\n",
    "        self.num_buckets_x = num_buckets_x\n",
    "        \n",
    "        self.min_x = env.unwrapped.min_position\n",
    "        self.max_x = env.unwrapped.max_position\n",
    "        \n",
    "        self.min_v = -env.unwrapped.max_speed\n",
    "        self.max_v = env.unwrapped.max_speed\n",
    "    \n",
    "        self.nS = self.num_buckets_v * self.num_buckets_x\n",
    "        \n",
    "    def get_bucket(self, vmin, vmax, v, num_buckets):\n",
    "        bucket_size = (vmax - vmin) / num_buckets\n",
    "        return int((v - vmin) // bucket_size)\n",
    "        \n",
    "    def __call__(self, state):\n",
    "        x, v = state\n",
    "        bucket_x = self.get_bucket(self.min_x, self.max_x, x, self.num_buckets_x)\n",
    "        bucket_v = self.get_bucket(self.min_v, self.max_v, v, self.num_buckets_v)\n",
    "        return int(bucket_x * self.num_buckets_v + bucket_v)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((self.num_buckets_v, self.num_buckets_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b413a04ca3749f6a58e8fde7d44f365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "car_state_encoder = MountainCairStateEncoder(car_env)\n",
    "Q_car, car_env = train_utils.cached_call(q_learning, 'MountainCar-v0', {}, {'num_episodes': 3 * 10**3, 'encode_state':car_state_encoder})\n",
    "p_car = train_utils.get_policy_from_q(Q_car, car_state_encoder.nS, car_env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards -1.0\n"
     ]
    }
   ],
   "source": [
    "train_utils.play_episode(car_env, p_car, encode_state=car_state_encoder, max_steps=2500)\n",
    "car_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿ Qué problemas/limitaciones tienen las técnicas vistas hasta ahora?\n",
    "* Los enfoques presentados hasta ahora tienen una representación discreta,\n",
    "* por lo tanto, necesitan mantener una representación explicita de cada estado.\n",
    "* Normalmente, la función a estimar (funcion value, policy, state-action) se almacena en un diccionario para cada estado.\n",
    "* Esta solución solo es válida en entornos donde el número de estados es reducidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poner en contexto esta limitación, consideremos los espacios de posibles acciones de los siguientes juegos:\n",
    "\n",
    "* Ajedrez: El número de posibles estados válidos se cree que está entre $10^{43}$ y $10^{50}$.\n",
    "* Go: En un tablero de 19x19, el número de estados es superior a $10^{172}$.\n",
    "* Animal crossing: Utilizando como observaciones la pantalla de la switch con resolución 1280x720 y una representación RGB, el número de estados sería $3^{921600}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como consecuencia, los métodos que hemos visto hasta ahora no son capaces de funcionar en este tipo de problemas (sin modificaciones):\n",
    "\n",
    "* En los métodos basados en programación dinámica, value iteration y policy iteration, las actualizaciones se realizan para cada uno de los estados, y se tiene una representación explicita para cada uno de ellos.\n",
    "* En los métodos basados en montecarlo y temporal-difference, las actualizaciones se realizan en base a la experiencia, y solo se actualizan aquellos pares de estados-acciones que se han visitado. Aún así, la función state-action (Q) no estaría definida en la mayoría de los casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo solucionamos este problema?:\n",
    "\n",
    "* Las representaciones de estados discretas asumen que los estados no guardan relación entre si.\n",
    "* En la mayoría de los casos esto no es así, y algunos estados son similares,\n",
    "* Y como consecuencia, las acciones resultantes serán similares.\n",
    "* **Solución**: estimar la función Q utilizando un estimador que sea capaz de explotar estas similaridades para encontrar una representación mas compacta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Y que técnica podemos utilizar para estimar esta función?:\n",
    "\n",
    "* Cualquiera de las técnicas de aprendizaje supervisado que hemos visto hasta ahora.\n",
    "* Recordad, que los modelos de aprendizaje supervisado tratan de estimar una función $f: X \\mapsto y$ utilizando otra función $\\hat{f}: X \\mapsto y$.\n",
    "* En este caso nos centremos en modelos de aprendizaje profundo debido a su habilidad para tratar con problemas de alta dimensionalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volviendo a Deep Q-Learning, ¿Qué necesitamos?:\n",
    "\n",
    "* El objetivo es estimar la función state-action Q → Modelo basado en Deep Learning.\n",
    "* Features del problema → El estado.\n",
    "* Etiquetas del problema → Los valores esperados de recompensa para cada una de las acciones.\n",
    "* Función de pérdida → ¿?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo se actualizan los valores en Q-Learning?\n",
    "* La actualización se realiza: $Q(s,a) =  Q(s, a) + \\alpha[R + \\gamma max_{a\\prime} Q(s\\prime, a\\prime) - Q(s, a)]$,\n",
    "* Y el término $R + \\gamma max_{a\\prime} Q(s\\prime, a\\prime) - Q(s, a)$ representa el error en la estimación.\n",
    "* **Función de pérdida** → $\\delta = Q(s, a) - (R + \\gamma max_{a\\prime} Q(s\\prime, a\\prime))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos todas las herramientas para utilizar Deep Q-Learning, ¿y ahora qué?:\n",
    "\n",
    "* En la práctica, cuando utilizamos un método off-policy, métodos de aproximación de funciones no lineales y boostrapping, el entrenamiento se vuelve inestable y puede que no converja.\n",
    "* Este problema es conocido como deadly triad (ver libro de Sutton & Barto).\n",
    "* El efecto de estos problemas se puede minimizar con **experience replay** y **fixed target**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experience replay**: \n",
    "\n",
    "* Guardamos una ventana de experiencias previas con el entorno, 4-uplas (estado, acción, proximo estado, recompensa)\n",
    "* En cada iteración, en lugar de entrenar con la última \"experiencia\", entrenamos con una muestra de tamaño aleatorio de la memoria de experiencias\n",
    "* Al ser muestras aleatorias y no consecutivas, minimizamos el efecto de correlaciones entre experiencias consecutivas.\n",
    "* Además, el método es más eficiente,\n",
    "* Y suaviza las actualizaciones, ya que estamos utilizando un conjunto de experiencias para actualizar los pesos, y no una experiencia sola."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fixed target**:\n",
    "\n",
    "* Como en cada step del entorno realizamos una iteración, los valores de la red neuronal que utilizamos para estimar Q varian.\n",
    "* Esto vuelve el modelo inestable y dificulta su convergencia (oscilaciones).\n",
    "* Para solucionar este problema se introduce una segunda red neuronal, **Fixed target**, igual a la red neuronal original y con los mismos parámetros.\n",
    "* La red original se entrena contra los valores obtenidos del modelo **Fixed target**, \n",
    "* Y periodicamente, los parámetros de la red original se copian en **Fixed target**.\n",
    "* Con esto se consigue hacer el entrenamiento mas estable y evitar las oscilaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora al código!\n",
    "\n",
    "Para implementar el módelo de Deep Q-Learning vamos a utilizar pytorch. Para una breve introducción de la librería ver: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html y la guia de instalación en https://pytorch.org/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que haremos será definir el dispositivo donde se almacenarán los tensores, y donde se realizarán las operaciones del modelo. En este caso, utilizaremos cuda si está disponible, en caso contrario, se ejecutarán en la cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Priemero definiremos la estructura de datos de replay memory, donde tendremos almacenadas las experiencias previas del modelo para su posterior entrenamiento. Esta estructura de datos se implementa mediante una lista circular, por lo que al llegar a su máxima capacidad, se irán sobreescribiendo los elementos por orden de antiguedad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory():\n",
    "    \"\"\"\n",
    "        Code from: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya podemos definir nuestro agente de Deep Q Learning:\n",
    "\n",
    "* El agente recibe como parámetros los modelos **target_net** y **policy_net**.\n",
    "* Cada **target_update** episodios, el modelo **target_net** se actualiza con los parámetros de **policy_net**,\n",
    "* Para ello, antes de cada episodio hay que llamar al método **start_episode()**.\n",
    "* Las computaciones se llevan a cabo en el dispositivo **device**.\n",
    "* Se almacenarán hasta **memory_size** 4-uplas de estado, acción, recompensa y nuevo estado, eliminando por orden de llegada cuando se llega a la capacidad total.\n",
    "* El número de acciones posibles para el agente es **n_actions**.\n",
    "* Para cada fase de entrenamiento parcial, se entrenará **policy_net** utilizando **batch_size** elementos de la memoria.\n",
    "* En caso de tener menos de **batch_size** elementos, no se entrenará.\n",
    "* El valor gammma se regula con el parámetro **gamma**.\n",
    "* Finalmente, se utilizará una policy epsilón-greedy, que se regula con los parámetros **eps_start**, **eps_end**, **eps_decay**,\n",
    "* calculándose el epsilón actual como: $\\epsilon = \\epsilon_{end} + (\\epsilon_{start} - \\epsilon_{end}) e^{-1 steps / \\epsilon_{decay}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y los métodos:\n",
    "\n",
    "* select_action: Selecciona una acción para el estado **state**  utilizando una policy epsilon-greedy con epsilon decay. En la evaluación, se utilizará el parámetro **trainning** para forzar una policy greedy.\n",
    "* step: Se almacena un step en la memoria (state, action, next_state, reward), y se llama al método **optiize_model**.\n",
    "* optimize_model: Se entrena el modelo. Sólo se optimizará cuando el tamaño de la memoria sería superior al tamaño de batch.\n",
    "* start_episode: Se debe ejecutar antes de empezar el episodio, y permite cada **target_update** episodios, actualizar el modelo **target_net**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "        Base code from (modified): https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, policy_net, target_net, device, n_actions, memory_size=10_000, batch_size=128, gamma=0.999, eps_start=0.9, eps_end=0.05, eps_decay=200, target_update=10):\n",
    "        self.target_net = target_net\n",
    "        self.policy_net = policy_net\n",
    "        self.optimizer = optim.RMSprop(policy_net.parameters())\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay = eps_decay\n",
    "        self.target_update = target_update\n",
    "        self.memory = ReplayMemory(memory_size)\n",
    "        self.device = device\n",
    "        self.steps_done = 0\n",
    "        self.i_episode = 0\n",
    "        \n",
    "    def select_action(self, state, trainning=True):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \\\n",
    "            math.exp(-1. * self.steps_done / self.eps_decay)\n",
    "        \n",
    "        if trainning:\n",
    "            self.steps_done += 1\n",
    "        \n",
    "        if sample > eps_threshold or not trainning:\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=self.device, dtype=torch.long)\n",
    "        \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                              batch.next_state)), device=self.device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        \n",
    "        # torch cat concatenates in the given dimension (by default 0)\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        \n",
    "        # Note: gather selects from the tensor self.policy_net(state_batch) with size batch_size x n_actions, \n",
    "        # the action that was actually selected for the experience. action_batch is the list of indexes of the selected actions\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        # Note: deatch returns a new Tensor, detached from the current graph\n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        # Unsqueeze returns a new tensor with a dimension of size one inserted at the specified position. (insert the batch dimension)\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def start_episode(self):\n",
    "        self.i_episode += 1\n",
    "        if self.i_episode % self.target_update == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.push(state, action, next_state, reward)\n",
    "        self.optimize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. \n",
    "* The system is controlled by applying a force of +1 or -1 to the cart. \n",
    "* The pendulum starts upright, and the goal is to prevent it from falling over. \n",
    "* A reward of +1 is provided for every timestep that the pole remains upright. \n",
    "* The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "Enlance al entorno: https://gym.openai.com/envs/CartPole-v1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"https://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/CartPole-v1/thumbnail.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"https://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/CartPole-v1/thumbnail.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creamos el estimador para la función Q del entorno cartpole utilizando DQN. \n",
    "* Dado que utilizaremos las imagenes del entorno el modelo será una red neuronal convolucional (podría ser una red fully connected)\n",
    "* En este caso, se utilizan 3 pares de convolución + batchnorm2d (Comentarios sobre el paper de batch norm https://gist.github.com/shagunsodhani/4441216a298df0fe6ab0)\n",
    "* Después, se utilizará una capa fully connected.\n",
    "* La activación en todas las capas será RELU. RELU(x) = max(0, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNCartPole(nn.Module):\n",
    "    \"\"\"\n",
    "        Code from: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQNCartPole, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de utilizar toda la imagen, se puede seleccionar solo aquella parte donde se encuentra el agente, disminuyendo el tamaño de entrada, y por tanto, la complejidad del problema (Fuente Code from: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "def get_cart_location(env, screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def get_screen(env, device):\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(env, screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    \n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "    \n",
    "    return resize(screen).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez ya tenemos definido el modelo y las transformaciones de la entrada (no son obligatorias):\n",
    "* Creamos el entorno.\n",
    "* Creamos el modelo **policy_net** y **target_net** y hacemos que **target_net** tenga los parámetros de **policy_net**,\n",
    "* Y creamos nuestro agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04293499, -0.02274823, -0.01172427, -0.02163571])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create environment\n",
    "cart_pole_env = gym.make('CartPole-v0').unwrapped\n",
    "cart_pole_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_screen = get_screen(cart_pole_env, device)\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = cart_pole_env.action_space.n\n",
    "\n",
    "# Create DQN models\n",
    "policy_net_cp = DQNCartPole(screen_height, screen_width, n_actions).to(device)\n",
    "target_net_cp = DQNCartPole(screen_height, screen_width, n_actions).to(device)\n",
    "target_net_cp.load_state_dict(policy_net_cp.state_dict())\n",
    "target_net_cp.eval()\n",
    "\n",
    "# Create agent\n",
    "cartpole_dqn_agent = DQNAgent(policy_net_cp, target_net_cp, device, n_actions, memory_size=10_000, eps_decay=200, target_update=10)\n",
    "\n",
    "# Close to env window\n",
    "cart_pole_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente podemos definir el bucle de entrenamiento, que entrenará el agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartpole_dqn(cart_pole_env, dqn_agent, num_episodes, train=True):\n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        # Initialize the environment and state\n",
    "        cart_pole_env.reset()\n",
    "        dqn_agent.start_episode()\n",
    "        last_screen = get_screen(cart_pole_env, device)\n",
    "        current_screen = get_screen(cart_pole_env, device)\n",
    "        state = current_screen - last_screen\n",
    "\n",
    "        for t in itertools.count():\n",
    "            # Select and perform an action\n",
    "            action = dqn_agent.select_action(state, train)\n",
    "            game_state, reward, done, _ = cart_pole_env.step(action.item())\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "\n",
    "            # Observe new state\n",
    "            last_screen = current_screen\n",
    "            current_screen = get_screen(cart_pole_env, device)\n",
    "            if not done:\n",
    "                next_state = current_screen - last_screen\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            if train:\n",
    "                # Store the transition and optimize\n",
    "                dqn_agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "    cart_pole_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y a entrenar! Podéis probar a cambiar los parámetros de la Red convolucional, añadiendo más filtros, capas, etc, o también tamaño del batch, gamma, y los parámetros de la política epsilon-greedy con epsilon-decay. \n",
    "\n",
    "**Importante**: Si ejecutáis la función para entrenar (**cartpole_dqn**), utilizará el estado del agente actual, no lo reinicia, por lo que sería como si siguiese el entrenamiento. Tened en cuenta que con policy epsilon-greedy con epsilon-decay llegará un punto que explorará muy poco, por lo que no aprenderá más. En ese caso, podéis crear otra vez el agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085848da4f904911bfb5812dd2f41fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cartpole_dqn(cart_pole_env, cartpole_dqn_agent, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3defcb97cb240309a5b5cbc86cedd1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cartpole_dqn(cart_pole_env, cartpole_dqn_agent, 10, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LunarLander-v2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. \n",
    "* Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. \n",
    "* Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. \n",
    "* Firing main engine is -0.3 points each frame. \n",
    "* Solved is 200 points. Landing outside landing pad is possible. \n",
    "* Fuel is infinite, so an agent can learn to fly and then land on its first attempt. \n",
    "* Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\n",
    "\n",
    "Enlance al entorno: https://gym.openai.com/envs/LunarLander-v2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"https://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/LunarLander-v2/original.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"https://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/LunarLander-v2/original.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creamos el estimador para la función Q del entorno cartpole utilizando DQN. \n",
    "* En este caso la entrada no es una imagen, por lo que utilizaremos capas fully connected.\n",
    "* Concretamente se utilizarán 3 capas, el tamaño de las dos primeras se puede regular con los parámetros **fc1_size** y **fc2_size**,\n",
    "* Mientras que el tamaño de la última será el número de acciones posibles **action_size**.\n",
    "* La activación en todas las capas será RELU. RELU(x) = max(0, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNLander(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed=0, fc1_size = 64, fc2_size = 64):\n",
    "        super(DQNLander, self).__init__()\n",
    "        hidden_size = 30\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.out = nn.Linear(fc2_size, action_size)\n",
    "        \n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso a diferencia del entorno CartPole-v0 no serán necesarias las transformaciones, sólo crear el agente, modelos y entorno:\n",
    "\n",
    "* Creamos el entorno.\n",
    "* Creamos el modelo policy_net y target_net y hacemos que target_net tenga los parámetros de policy_net,\n",
    "* Y creamos nuestro agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/rl_lecture/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "lunar_env = gym.make('LunarLander-v2')\n",
    "state = lunar_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = lunar_env.action_space.n\n",
    "\n",
    "policy_net_ll = DQNLander(len(state), n_actions).to(device)\n",
    "target_net_ll = DQNLander(len(state), n_actions).to(device)\n",
    "target_net_ll.load_state_dict(policy_net_ll.state_dict())\n",
    "target_net_ll.eval()\n",
    "\n",
    "dqn_agent_lander = DQNAgent(policy_net_ll, target_net_ll, device, n_actions, memory_size=5_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lunar_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El bucle de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lander_dqn(lunar_env, dqn_agent_lander, num_episodes, train=True, render=True, sleep=None):\n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        # Initialize the environment and state\n",
    "        state = torch.from_numpy(lunar_env.reset()).to(device).view(1, -1)\n",
    "        dqn_agent_lander.start_episode()\n",
    "\n",
    "        for t in itertools.count():\n",
    "            # Select and perform an action\n",
    "            action = dqn_agent_lander.select_action(state, trainning=train)\n",
    "            game_state, reward, done, _ = lunar_env.step(action.item())\n",
    "            reward = torch.tensor([float(reward)], device=device)\n",
    "            \n",
    "            if render:\n",
    "                lunar_env.render()\n",
    "\n",
    "            game_state = torch.from_numpy(game_state).to(device).view(1, -1)\n",
    "            \n",
    "            # Observe new state\n",
    "            if not done:\n",
    "                next_state = game_state\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition and optimize\n",
    "            if train:\n",
    "                dqn_agent_lander.step(state, action, reward, next_state, done)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            \n",
    "            if sleep:\n",
    "                time.sleep(sleep)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "    lunar_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A entrenar! (si desactivamos el render irá más rápido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0aed06167e4a93bfc445f8fcfea439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lander_dqn(lunar_env, dqn_agent_lander, 400, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar sin entrenar el agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8795fb61ccf4e04babe01864fe72512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lander_dqn(lunar_env, dqn_agent_lander, 1, train=False, sleep=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LunarLander-v2 con imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, probaremos el entorno anterior pero utilizando la imagen del entorno en lugar del estado.\n",
    "\n",
    "En este caso utilizarmeos la pantalla completa, ya que el agente puede desplazarse por ella. Para ello, modificaremos la función del experimento de CartPole-v0 para convertirla en un tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_screen(env, device):\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "    return resize(screen).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el modelo, en este caso al igual que en CartPole-v0 tenemos como entrada una imagen. Se ha reutilizado el modelo pero añadiendo una capa más fully connected antes de la capa de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDQNLander(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs, fc1=64):\n",
    "        super(GDQNLander, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        \n",
    "        self.fc1  = nn.Linear(linear_input_size, fc1)\n",
    "        self.head = nn.Linear(fc1, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00398741,  1.409437  ,  0.40385824, -0.06592722, -0.00461354,\n",
       "       -0.09147988,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lunar_env = gym.make('LunarLander-v2')\n",
    "lunar_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos los modelos y el agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_screen = get_full_screen(lunar_env, device)\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = lunar_env.action_space.n\n",
    "\n",
    "policy_net_llg = GDQNLander(screen_height, screen_width, n_actions).to(device)\n",
    "target_net_llg = GDQNLander(screen_height, screen_width, n_actions).to(device)\n",
    "target_net_llg.load_state_dict(policy_net_llg.state_dict())\n",
    "target_net_llg.eval()\n",
    "\n",
    "lunar_env.close()\n",
    "\n",
    "gdqn_agent_lander = DQNAgent(policy_net_llg, target_net_llg, device, n_actions, memory_size=30_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El bucle de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lander_img_dqn(lunar_env, dqn_agent_lander, num_episodes, train=True):\n",
    "    for i_episode in tqdm(range(num_episodes)):\n",
    "        # Initialize the environment and state\n",
    "        lunar_env.reset()\n",
    "        dqn_agent_lander.start_episode()\n",
    "        state = get_full_screen(lunar_env, device)\n",
    "\n",
    "        for t in itertools.count():\n",
    "            # Select and perform an action\n",
    "            action = dqn_agent_lander.select_action(state)\n",
    "            game_state, reward, done, _ = lunar_env.step(action.item())\n",
    "            reward = torch.tensor([float(reward)], device=device)\n",
    "\n",
    "            # Observe new state\n",
    "            current_screen = get_full_screen(lunar_env, device)\n",
    "            if not done:\n",
    "                next_state = current_screen\n",
    "            else:\n",
    "                next_state = None\n",
    "\n",
    "            # Store the transition and optimize\n",
    "            dqn_agent_lander.step(state, action, reward, next_state, done)\n",
    "\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "    lunar_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A entrenar!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8462ca59ad2143ee99a6b9f7b0a91b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lander_img_dqn(lunar_env, gdqn_agent_lander, num_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lander_img_dqn(lunar_env, gdqn_agent_lander, num_episodes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias:\n",
    "\n",
    "* Libro de Sutton & Barto, no es muy complejo y está abierto en: http://incompleteideas.net/book/bookdraft2017nov5.pdf\n",
    "* Diapositivas de Silver: https://www.davidsilver.uk/teaching/\n",
    "* Estado del arte de aprendizaje reforzado: https://arxiv.org/pdf/1708.05866.pdf\n",
    "* Tutorial online: https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#deadly-triad-issue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
